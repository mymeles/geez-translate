version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        - MODEL_SIZE=${MODEL_SIZE:-large}  # Default to large if not specified
    restart: always
    ports:
      - "8000:8000"
    environment:
      - PORT=8000
      - WORKERS=1  # Single worker for better GPU utilization
      - BATCH_SIZE=${BATCH_SIZE:-16}  # Optimized for L4 GPU
      - MAX_AUDIO_LENGTH=300
      # Use the local model path inside the container with dynamic model size
      - MODEL_SIZE=${MODEL_SIZE:-large}
      - MODEL_PATH=/app/models/seamless-m4t-v2-${MODEL_SIZE:-large}
      - LOAD_MODEL_ON_STARTUP=true
      - TRANSFORMERS_CACHE=/app/models/cache
      - HF_HOME=/app/models/cache
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - PYTHONUNBUFFERED=1
      # Fix for CUDA multiprocessing
      - PYTORCH_MULTIPROCESSING_START_METHOD=spawn
      # Start in online mode for model download, then switch to offline later
      - TRANSFORMERS_OFFLINE=${TRANSFORMERS_OFFLINE:-0}
      # Optimization environment variables
      - USE_QUANTIZATION=${USE_QUANTIZATION:-true}
      - USE_TORCH_COMPILE=${USE_TORCH_COMPILE:-true}
      - CHUNK_SIZE=${CHUNK_SIZE:-5}
      - CHUNK_OVERLAP=${CHUNK_OVERLAP:-0.5}
    volumes:
      - huggingface_cache:/app/models/cache
      # Mount local model directory 
      - ./models:/app/models
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Modified command to check if model exists and download if needed
    command: >
      bash -c "
        MODEL_DIR=/app/models/seamless-m4t-v2-${MODEL_SIZE:-large}
        CONFIG_FILE=$${MODEL_DIR}/config.json
        PROCESSOR_CONFIG=$${MODEL_DIR}/processor_config.json
        TOKENIZER_CONFIG=$${MODEL_DIR}/tokenizer_config.json
        FEATURE_EXTRACTOR=$${MODEL_DIR}/feature_extractor_config.json

        # Check if model and processor exist in the container
        if [ ! -f $${CONFIG_FILE} ] || ([ ! -f $${PROCESSOR_CONFIG} ] && [ ! -f $${TOKENIZER_CONFIG} ] && [ ! -f $${FEATURE_EXTRACTOR} ]); then
          echo '--------------------------------------------------------------'
          echo 'Model or processor not found, downloading...'
          echo '--------------------------------------------------------------'
          
          # Ensure online mode during download
          export TRANSFORMERS_OFFLINE=0
          
          # Download model with processor
          python download_model.py --model-id facebook/seamless-m4t-v2-${MODEL_SIZE:-large} --cache-dir $${MODEL_DIR} --force
          
          # Check if download was successful
          if [ -f $${CONFIG_FILE} ] && ([ -f $${PROCESSOR_CONFIG} ] || [ -f $${TOKENIZER_CONFIG} ] || [ -f $${FEATURE_EXTRACTOR} ]); then
            echo '--------------------------------------------------------------'
            echo 'Model and processor download complete, switching to offline mode'
            echo '--------------------------------------------------------------'
            export TRANSFORMERS_OFFLINE=1
          else
            echo '--------------------------------------------------------------'
            echo 'WARNING: Model download may not be complete, check logs for errors'
            echo 'Directory contents:'
            ls -la $${MODEL_DIR}
            echo '--------------------------------------------------------------'
            # Stay in online mode to allow fallback
            export TRANSFORMERS_OFFLINE=0
          fi
        else
          echo '--------------------------------------------------------------'
          echo 'Model and processor already exist'
          echo '--------------------------------------------------------------'
          ls -la $${MODEL_DIR} | head -20
          echo '--------------------------------------------------------------'
          echo 'Setting offline mode'
          export TRANSFORMERS_OFFLINE=1
        fi
        
        # Start the application
        echo '--------------------------------------------------------------'
        echo 'Starting API server'
        echo '--------------------------------------------------------------'
        # Better configuration for uvicorn in production
        python -m uvicorn app:app --host 0.0.0.0 --port 8000 --log-level ${LOG_LEVEL:-info} --timeout-keep-alive 75 --lifespan on"

  # Optional Nginx reverse proxy with enhanced caching
  nginx:
    image: nginx:latest
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
    # Add custom cache configuration for NGINX
    environment:
      - NGINX_MAX_CACHE=1g
      - NGINX_CACHE_VALID=60m

volumes:
  huggingface_cache:
    # Named volume for persistent storage