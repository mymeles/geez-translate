version: '3.8'

services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    ports:
      - "8000:8000"
    environment:
      - PORT=8000
      - WORKERS=1  # Single worker for better GPU utilization
      - BATCH_SIZE=16  # Optimized for L4 GPU
      - MAX_AUDIO_LENGTH=300
      # Use the local model path inside the container
      - MODEL_PATH=/app/models/seamless-m4t-v2-medium
      - LOAD_MODEL_ON_STARTUP=true
      - TRANSFORMERS_CACHE=/home/appuser/.cache/huggingface
      - HF_HOME=/home/appuser/.cache/huggingface
      - LOG_LEVEL=INFO  # Changed from DEBUG to reduce logging overhead
      - PYTHONUNBUFFERED=1
      # Optimization environment variables
      - USE_QUANTIZATION=true
      - USE_TORCH_COMPILE=true
      - CHUNK_SIZE=5
      - CHUNK_OVERLAP=0.5
    volumes:
      - huggingface_cache:/home/appuser/.cache/huggingface
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 60s  # Increased timeout for model loading
      retries: 5
      start_period: 60s  # Give more time for initial model loading

  # Optional Nginx reverse proxy with enhanced caching
  nginx:
    image: nginx:latest
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - api
    # Add custom cache configuration for NGINX
    environment:
      - NGINX_MAX_CACHE=1g
      - NGINX_CACHE_VALID=60m

volumes:
  huggingface_cache:
    # Named volume for persistent storage